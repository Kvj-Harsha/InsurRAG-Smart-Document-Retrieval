{
  "project": {
    "name": "Fast RAG Chatbot API",
    "description": "An industry-grade, low-latency Retrieval-Augmented Generation (RAG) chatbot API using Groq LLMs, deployed on Fly.io.",
    "version": "2.0.0"
  },
  "deployment": {
    "platform": "Fly.io",
    "features": ["Fast cold start", "Global edge locations", "Docker support"],
    "ci_cd": "GitHub Actions",
    "docker_compose": true
  },
  "tech_stack": {
    "language": "Python 3.11+",
    "framework": "FastAPI (async)",
    "llm": "Groq Llama 3 70B via groq-sdk",
    "parser": "pypdf2",
    "chunking": "LangChain RecursiveCharacterTextSplitter",
    "embedding_model": "Nomic Embed v1.5 (async)",
    "vector_db": "Weaviate (external or self-hosted)",
    "cache": "Redis (aioredis)",
    "concurrency": "asyncio, httpx",
    "logging": "Uvicorn structured logging",
    "metrics": "Prometheus (optional)",
    "tracing": "OpenTelemetry (optional)",
    "testing": ["pytest", "httpx", "locust"]
  },
  "file_structure": {
    "root": [
      "Dockerfile",
      "docker-compose.yml",
      "fly.toml",
      "README.md",
      "requirements.txt"
    ],
    "app": {
      "main.py": "Entry point with FastAPI app and routing",
      "api": {
        "routes.py": "API endpoint definitions",
        "schemas.py": "Request and response models"
      },
      "services": {
        "parser.py": "Unstructured file parsing logic",
        "embedder.py": "Embedding generation using Nomic",
        "retriever.py": "Weaviate-based similarity search",
        "llm.py": "Prompt construction and Groq LLM call",
        "cache.py": "Redis caching logic"
      },
      "utils": {
        "logger.py": "Custom structured logger setup",
        "config.py": "Environment variables, constants"
      },
      "tests": {
        "test_api.py": "Integration tests for /query",
        "test_embedder.py": "Unit tests for embedder",
        "test_parser.py": "Unit tests for file parsing"
      }
    }
  },
  "api_specification": {
    "/query": {
      "method": "POST",
      "description": "Accepts a file (.pdf, .docx, .eml, .msg) and a natural language question. Returns an answer from the document.",
      "request": {
        "multipart/form-data": {
          "file": "binary (.pdf, .docx, .eml, .msg)",
          "question": "string"
        }
      },
      "response": {
        "200": {
          "answer": "string",
          "source_chunks": ["string"],
          "latency_ms": "integer"
        },
        "400": {
          "error": "Invalid file format or missing input"
        },
        "500": {
          "error": "Internal server or LLM failure"
        }
      }
    }
  },
  "performance": {
    "target_latency_ms": 1000,
    "max_doc_size_mb": 20,
    "expected_rps": 50,
    "max_concurrent": 100
  },
  "security": {
    "file_upload_validation": true,
    "rate_limiting": "Optional (SlowAPI)",
    "authentication": "Optional API key",
    "redacted_logs": true
  },
  "testing": {
    "unit_tests": true,
    "integration_tests": true,
    "performance_tests": "Locust",
    "security_tests": "Optional with scanners (Bandit, Trivy)"
  },
  "monitoring": {
    "logs": "Structured JSON logs",
    "metrics": "Prometheus/Grafana (optional)",
    "tracing": "OpenTelemetry (optional)"
  },
  "glossary": {
    "RAG": "Retrieval-Augmented Generation",
    "LLM": "Large Language Model",
    "API": "Application Programming Interface",
    "Weaviate": "Vector database for similarity search",
    "Fly.io": "Fast edge-hosted deployment platform"
  }
}
